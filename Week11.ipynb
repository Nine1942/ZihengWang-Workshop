{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bi49hm\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\generation\\configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\bi49hm\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\generation\\configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      "Give me a short introduction to large language models.\n",
      "\n",
      "The first thing you need to know is that the language model is a set of rules that you can use to define your own language.\n",
      "\n",
      "The rules are:\n",
      "\n",
      "A language is a set of rules that you can use to define your own language.\n",
      "\n",
      "A language is a set of rules that you can use to define your own language. A language is a set of rules that you can use to define your own language. A language is a set of rules that you can use to define your own language. A language is a set of rules that you can use to define your own language. A language is a set of rules that you can use to define your own language. A language is a set of rules that you\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"gpt2\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "prompt = \"Give me a short introduction to large language models.\"\n",
    "\n",
    "model_inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "generation_params = {\"temperature\": 0.7, \"top_k\": 50, \"top_p\": 0.9, \"max_new_tokens\": 150}\n",
    "\n",
    "generated_ids = model.generate(**model_inputs, **generation_params)\n",
    "\n",
    "response = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "print(f\"Response:\\n{response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33bc568cdae345298939bfbb30e5cb76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/988M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bi49hm\\AppData\\Roaming\\Python\\Python312\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\bi49hm\\.cache\\huggingface\\hub\\models--Qwen--Qwen2.5-0.5B-Instruct. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92808b624d2e4c0f9414bd4c8312881f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/242 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48e817d10edc4c2c8fe5cc2cb2d4186d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/7.30k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f575ab1b0f5d45b6b11c065294092b5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10bea16da3544af793b7213f4a3023c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "646d219a1b1f460c859369ad898a006f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      "system\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\n",
      "user\n",
      "Give me a short introduction to large language models.\n",
      "assistant\n",
      "Large language models (LLMs) refer to artificial intelligence systems that can generate human-like text based on a set of input data or prompts. These models have been around for decades and have become increasingly sophisticated in recent years.\n",
      "\n",
      "One of the key characteristics of LLMs is their ability to understand and generate complex natural language tasks such as generating code, writing articles, creating speeches, and more. They can also learn from examples and feedback, allowing them to improve over time.\n",
      "\n",
      "LLMs are often used in fields such as machine translation, sentiment analysis, and question answering. They are also being explored in areas like speech recognition and gaming, where they may be able to outperform humans in certain tasks.\n",
      "\n",
      "In addition to their technical capabilities, LLMs are\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=\"auto\", device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "prompt = \"Give me a short introduction to large language models.\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "\n",
    "input_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "model_inputs = tokenizer([input_text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "generation_params = {\"temperature\": 0.7, \"top_k\": 50, \"top_p\": 0.9, \"max_new_tokens\": 150}\n",
    "\n",
    "generated_ids = model.generate(**model_inputs, **generation_params)\n",
    "\n",
    "response = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "print(f\"Response:\\n{response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Experiment 1: Parameters {'temperature': 0.7, 'top_k': 50, 'top_p': 0.9, 'max_new_tokens': 150} ###\n",
      "Response:\n",
      "Large language models (LLMs) refer to artificial intelligence systems that can generate human-like text using natural language processing techniques. These models have been trained on vast amounts of data and can understand and generate complex sentences, as well as convey a wide range of topics and emotions.\n",
      "\n",
      "The term \"large\" in LLMs typically refers to the scale at which these models operate. They may be capable of generating text for specific tasks or applications, such as writing articles, creating presentations, or answering questions from humans. Some LLMs, like GPT-3, can even generate new content without explicit instruction, making them highly versatile tools for various fields.\n",
      "\n",
      "One of the key benefits of LLMs is their ability to handle a wide range of tasks efficiently\n",
      "\n",
      "### Experiment 2: Parameters {'temperature': 1.0, 'top_k': 0, 'top_p': 0.8, 'max_new_tokens': 150} ###\n",
      "Response:\n",
      "Sure! Large language models (LLMs) refer to artificial intelligence systems that can produce human-like language outputs. These models often utilize deep neural networks and are trained on vast amounts of text data, including books, documents, websites, social media posts, etc. They can learn patterns in the natural language and generate coherent responses similar to humans.\n",
      "\n",
      "Llama 2 is one of the most popular LLMs developed by Alibaba Cloud. It has been designed for various tasks such as summarization, question answering, sentiment analysis, and more. By training it with large volumes of domain-specific data, it excels at generating high-quality results even when dealing with complex questions or answers.\n",
      "\n",
      "These advanced AI tools have transformed industries like journalism, legal services, customer support\n",
      "\n",
      "### Experiment 3: Parameters {'temperature': 0.3, 'top_k': 10, 'top_p': 1.0, 'max_new_tokens': 150} ###\n",
      "Response:\n",
      "Large Language Models (LLMs) are artificial intelligence systems designed to generate human-like text based on input data. These models can be trained on vast amounts of text data and learn to understand complex patterns in the language. LLMs have been used in a wide range of applications, including natural language processing, machine translation, chatbots, and more. They are often compared to humans in terms of their ability to produce coherent and contextually relevant responses.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "prompt = \"Give me a short introduction to large language models.\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "parameters = [\n",
    "    {\"temperature\": 0.7, \"top_k\": 50, \"top_p\": 0.9, \"max_new_tokens\": 150},\n",
    "    {\"temperature\": 1.0, \"top_k\": 0, \"top_p\": 0.8, \"max_new_tokens\": 150},\n",
    "    {\"temperature\": 0.3, \"top_k\": 10, \"top_p\": 1.0, \"max_new_tokens\": 150},\n",
    "]\n",
    "\n",
    "for i, params in enumerate(parameters):\n",
    "    print(f\"\\n### Experiment {i + 1}: Parameters {params} ###\")\n",
    "    generated_ids = model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=params[\"max_new_tokens\"],\n",
    "        temperature=params[\"temperature\"],\n",
    "        top_k=params[\"top_k\"],\n",
    "        top_p=params[\"top_p\"]\n",
    "    )\n",
    "    \n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    response = tokenizer.batch_decode(generated_ids,skip_special_tokens=True)[0]\n",
    "    print(f\"Response:\\n{response}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
